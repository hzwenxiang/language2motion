{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpbbeal5d9/swift-install\n",
      "[1/5] Compiling STBImage stb_image_write.c\n",
      "[2/5] Compiling Batcher Backend.swift\n",
      "[3/5] Compiling STBImage stb_image.c\n",
      "[4/5] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "/notebooks/language2motion.gt/swift-install/package/.build/checkouts/swift-protobuf/Sources/SwiftProtobuf/BinaryDelimited.swift:198:7: warning: variable 'readBuffer' was never mutated; consider changing to 'let' constant\n",
      "  var readBuffer = UnsafeMutablePointer<UInt8>.allocate(capacity: 1)\n",
      "  ~~~ ^\n",
      "  let\n",
      "[5/6] Compiling ModelSupport BijectiveDictionary.swift\n",
      "[6/7] Compiling Datasets BostonHousing.swift\n",
      "[7/8] Compiling TextModels Attention.swift\n",
      "[8/9] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[9/9] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: \n",
    "// + load csv\n",
    "// + split into train/dev\n",
    "// + convert to [Example]\n",
    "// + get sorted labels\n",
    "// - integrate with Language2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pd  = Python.import(\"pandas\")\n",
    "let model_selection  = Python.import(\"sklearn.model_selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   sample_id  ...                      label\n",
       "0          1  ...         Walking or running\n",
       "1          2  ...  Walking forward few steps\n",
       "2          3  ...         Walking or running\n",
       "3          4  ...         Walking or running\n",
       "4          5  ...  Walking forward few steps\n",
       "\n",
       "[5 rows x 3 columns]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let df = pd.read_csv(dsURL.path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array<String>\n",
      "String\n"
     ]
    }
   ],
   "source": [
    "let labels: [String] = df.label.unique().sorted().map {String($0)!}\n",
    "print(type(of:labels))\n",
    "print(type(of:labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Example {\n",
    "    public let id: String\n",
    "    public let text: String\n",
    "    public let label: String?\n",
    "\n",
    "    public init(id: String, text: String, label: String?) {\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      sample_id  ...                      label\n",
       "1592       1621  ...         Walking or running\n",
       "1699       1728  ...  Walking forward few steps\n",
       "2555       3071  ...         Walking or running\n",
       "2082       2111  ...            Doing something\n",
       "2402       2758  ...            Doing something\n",
       "...         ...  ...                        ...\n",
       "2446       2867  ...         Walking or running\n",
       "1102       1110  ...            Doing something\n",
       "2678       3304  ...         Walking or running\n",
       "1477       1485  ...         Walking or running\n",
       "717         724  ...  Walking forward few steps\n",
       "\n",
       "[2409 rows x 3 columns]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let (train_df, test_df) = model_selection.train_test_split(df, test_size: 0.2).tuple2\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "func Df2Example(df: PythonObject) -> [Example] {\n",
    "    return Python.list(df.iterrows()).map {\n",
    "        (rowObj: PythonObject) -> Example in \n",
    "        let row = rowObj.tuple2.1\n",
    "        let sample_id: String = \"\\(row.sample_id)\" // Int to String\n",
    "        let text: String = String(row.text)!\n",
    "        let label: String? = String(row.label)\n",
    "        return Example(id: sample_id, text: text, label: label)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ .0 : 3 elements\n",
       "    ▿ 0 : Example\n",
       "      - id : \"1621\"\n",
       "      - text : \"A person is walking carefully avoiding obstacles.\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Walking or running\"\n",
       "    ▿ 1 : Example\n",
       "      - id : \"1728\"\n",
       "      - text : \"A person stumbles slightly while walking backwards but quickly recovers and continues moving backwards.\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Walking forward few steps\"\n",
       "    ▿ 2 : Example\n",
       "      - id : \"3071\"\n",
       "      - text : \"A human is hitting something\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Walking or running\"\n",
       "  ▿ .1 : 3 elements\n",
       "    ▿ 0 : Example\n",
       "      - id : \"1109\"\n",
       "      - text : \"Subject starts a halfhearted jog.\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Doing something\"\n",
       "    ▿ 1 : Example\n",
       "      - id : \"1992\"\n",
       "      - text : \"A person walks forwards, turns around a walks back.\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Walking forward few steps\"\n",
       "    ▿ 2 : Example\n",
       "      - id : \"1557\"\n",
       "      - text : \"A human walks forward for seven steps and then stops.\"\n",
       "      ▿ label : Optional<String>\n",
       "        - some : \"Doing something\"\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let trainExamples = Df2Example(df: train_df.iloc[0..<3])\n",
    "let testExamples = Df2Example(df: test_df.iloc[0..<3])\n",
    "(trainExamples, testExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Language2Label {\n",
    "    public let directoryURL: URL\n",
    "    public let trainExamples: [Example]\n",
    "    public let devExamples: [Example]\n",
    "    public let testExamples: [Example]\n",
    "    public let maxSequenceLength: Int\n",
    "    public let batchSize: Int\n",
    "    // TODO: add labels\n",
    "\n",
    "    public typealias ExampleIterator = IndexingIterator<[Example]>\n",
    "    public typealias TrainDataIterator = PrefetchIterator<\n",
    "        GroupedIterator<MapIterator<ExampleIterator, DataBatch>>\n",
    "    >\n",
    "    public typealias DevDataIterator = GroupedIterator<MapIterator<ExampleIterator, DataBatch>>\n",
    "    public typealias TestDataIterator = DevDataIterator\n",
    "\n",
    "    public var trainDataIterator: TrainDataIterator\n",
    "    public var devDataIterator: DevDataIterator\n",
    "    public var testDataIterator: TestDataIterator\n",
    "}\n",
    "\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "// Data\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "\n",
    "extension Language2Label {\n",
    "    /// Language2Label example.\n",
    "    public struct Example {\n",
    "        public let id: String\n",
    "        public let sentence: String // change to annotation\n",
    "        public let isAcceptable: Bool? // TODO: change to 5 labels\n",
    "\n",
    "        public init(id: String, sentence: String, isAcceptable: Bool?) {\n",
    "            self.id = id\n",
    "            self.sentence = sentence\n",
    "            self.isAcceptable = isAcceptable\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// Language2Label data batch.\n",
    "    public struct DataBatch: KeyPathIterable {\n",
    "        public var inputs: TextBatch  // TODO: !!! Mutable in order to allow for batching.\n",
    "        public var labels: Tensor<Int32>?  // TODO: !!! Mutable in order to allow for batching.\n",
    "\n",
    "        public init(inputs: TextBatch, labels: Tensor<Int32>?) {\n",
    "            self.inputs = inputs\n",
    "            self.labels = labels\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// URL pointing to the downloadable ZIP file that contains the CoLA dataset.\n",
    "    private static let url: URL = URL(\n",
    "        string: String(\n",
    "            \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/\"\n",
    "                + \"o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\"))!\n",
    "\n",
    "    internal enum FileType: String {\n",
    "        case train = \"train\"\n",
    "        case dev = \"dev\"\n",
    "        case test = \"test\" // TODO: kill\n",
    "    }\n",
    "    // TODO: load csv file, split into train/test, extract row annotation -> sentence, label -> label\n",
    "    internal static func load(fromFile fileURL: URL, fileType: FileType) throws -> [Example] {\n",
    "        let lines = try parse(tsvFileAt: fileURL)\n",
    "\n",
    "        if fileType == .test {\n",
    "            // The test data file has a header.\n",
    "            return lines.dropFirst().enumerated().map { (i, lineParts) in\n",
    "                Example(id: lineParts[0], sentence: lineParts[1], isAcceptable: nil)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return lines.enumerated().map { (i, lineParts) in\n",
    "            Example(id: lineParts[0], sentence: lineParts[3], isAcceptable: lineParts[1] == \"1\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "internal func parse(tsvFileAt fileURL: URL) throws -> [[String]] {\n",
    "    try Data(contentsOf: fileURL).withUnsafeBytes {\n",
    "        $0.split(separator: UInt8(ascii: \"\\n\")).map {\n",
    "            $0.split(separator: UInt8(ascii: \"\\t\"), omittingEmptySubsequences: false)\n",
    "                .map { String(decoding: UnsafeRawBufferPointer(rebasing: $0), as: UTF8.self) }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "extension Language2Label {\n",
    "    public init(\n",
    "        exampleMap: @escaping (Example) -> DataBatch,\n",
    "        taskDirectoryURL: URL,\n",
    "        maxSequenceLength: Int,\n",
    "        batchSize: Int,\n",
    "        dropRemainder: Bool\n",
    "    ) throws {\n",
    "        self.directoryURL = taskDirectoryURL.appendingPathComponent(\"CoLA\")\n",
    "        let dataURL = directoryURL.appendingPathComponent(\"data\")\n",
    "        let compressedDataURL = dataURL.appendingPathComponent(\"downloaded-data.zip\")\n",
    "\n",
    "        // Download the data, if necessary.\n",
    "        try download(from: Language2Label.url, to: compressedDataURL)\n",
    "\n",
    "        // Extract the data, if necessary.\n",
    "        let extractedDirectoryURL = compressedDataURL.deletingPathExtension()\n",
    "        if !FileManager.default.fileExists(atPath: extractedDirectoryURL.path) {\n",
    "            try extract(zipFileAt: compressedDataURL, to: extractedDirectoryURL)\n",
    "        }\n",
    "\n",
    "        #if false\n",
    "            // FIXME: Need to generalize `DatasetUtilities.downloadResource` to accept\n",
    "            // arbitrary full URLs instead of constructing full URL from filename and\n",
    "            // file extension.\n",
    "            DatasetUtilities.downloadResource(\n",
    "                filename: \"\\(subDirectory)\", fileExtension: \"zip\",\n",
    "                remoteRoot: url.deletingLastPathComponent(),\n",
    "                localStorageDirectory: directory)\n",
    "        #endif\n",
    "\n",
    "        // Load the data files into arrays of examples.\n",
    "        let dataFilesURL = extractedDirectoryURL.appendingPathComponent(\"CoLA\")\n",
    "        self.trainExamples = try Language2Label.load(\n",
    "            fromFile: dataFilesURL.appendingPathComponent(\"train.tsv\"),\n",
    "            fileType: .train)\n",
    "        self.devExamples = try Language2Label.load(\n",
    "            fromFile: dataFilesURL.appendingPathComponent(\"dev.tsv\"),\n",
    "            fileType: .dev)\n",
    "        self.testExamples = try Language2Label.load(\n",
    "            fromFile: dataFilesURL.appendingPathComponent(\"test.tsv\"),\n",
    "            fileType: .test)\n",
    "\n",
    "        self.maxSequenceLength = maxSequenceLength\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        // Create the data iterators used for training and evaluating.\n",
    "        self.trainDataIterator = trainExamples.shuffled().makeIterator()  // TODO: [RNG] Seed support.\n",
    "            .map(exampleMap)\n",
    "            .grouped(\n",
    "                keyFn: { _ in 0 },\n",
    "                sizeFn: { _ in batchSize / maxSequenceLength },\n",
    "                reduceFn: {\n",
    "                    DataBatch(\n",
    "                        inputs: padAndBatch(\n",
    "                            textBatches: $0.map { $0.inputs }, maxLength: maxSequenceLength),\n",
    "                        labels: Tensor.batch($0.map { $0.labels! }))\n",
    "                },\n",
    "                dropRemainder: dropRemainder\n",
    "            )\n",
    "            .prefetched(count: 2)\n",
    "        self.devDataIterator = devExamples.makeIterator()\n",
    "            .map(exampleMap)\n",
    "            .grouped(\n",
    "                keyFn: { _ in 0 },\n",
    "                sizeFn: { _ in batchSize / maxSequenceLength },\n",
    "                reduceFn: {\n",
    "                    DataBatch(\n",
    "                        inputs: padAndBatch(\n",
    "                            textBatches: $0.map { $0.inputs }, maxLength: maxSequenceLength),\n",
    "                        labels: Tensor.batch($0.map { $0.labels! }))\n",
    "                },\n",
    "                dropRemainder: dropRemainder\n",
    "            )\n",
    "        self.testDataIterator = testExamples.makeIterator()\n",
    "            .map(exampleMap)\n",
    "            .grouped(\n",
    "                keyFn: { _ in 0 },\n",
    "                sizeFn: { _ in batchSize / maxSequenceLength },\n",
    "                reduceFn: {\n",
    "                    DataBatch(\n",
    "                        inputs: padAndBatch(\n",
    "                            textBatches: $0.map { $0.inputs }, maxLength: maxSequenceLength),\n",
    "                        labels: nil)\n",
    "                },\n",
    "                dropRemainder: dropRemainder\n",
    "            )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "// Regarding the batch size, note that the way batching is performed currently is that we bucket\n",
    "// input sequences based on their length (e.g., first bucket contains sequences of length 1 to 10,\n",
    "// second 11 to 20, etc.). We then keep processing examples in the input data pipeline until a\n",
    "// bucket contains enough sequences to form a batch. The batch size specified in the task\n",
    "// constructor specifies the *total number of tokens in the batch* and not the total number of\n",
    "// sequences. So, if the batch size is set to 1024, the first bucket (i.e., lengths 1 to 10)\n",
    "// will need 1024 / 10 = 102 examples to form a batch (every sentence in the bucket is padded\n",
    "// to the max length of the bucket). This kind of bucketing is common practice with NLP models and\n",
    "// it is done to improve memory usage and computational efficiency when dealing with sequences of\n",
    "// varied lengths. Note that this is not used in the original BERT implementation released by\n",
    "// Google and so the batch size setting here is expected to differ from that one.\n",
    "let maxSequenceLength = 128\n",
    "let batchSize = 1024\n",
    "\n",
    "// Create a function that converts examples to data batches.\n",
    "let exampleMapFn: (Language2Label.Example) -> Language2Label.DataBatch = { example -> Language2Label.DataBatch in\n",
    "    let textBatch = bertClassifier.bert.preprocess(\n",
    "        sequences: [example.sentence],\n",
    "        maxSequenceLength: maxSequenceLength)\n",
    "    return Language2Label.DataBatch( // TODO: get label idx\n",
    "        inputs: textBatch, labels: example.isAcceptable.map { Tensor($0 ? 1 : 0) })\n",
    "}\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "    exampleMap: exampleMapFn,\n",
    "    taskDirectoryURL: workspaceURL,\n",
    "    maxSequenceLength: maxSequenceLength,\n",
    "    batchSize: batchSize,\n",
    "    dropRemainder: true)\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ bert_models/CoLA/ -- file:///tmp/\n",
       "  ▿ _storage : Optional<Storage>\n",
       "    ▿ some : Storage\n",
       "      ▿ boxedRequiresCopyOnWrite : <URLBox: 0x366ae80>\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.directoryURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8551\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ Example\n",
       "  - id : \"gj04\"\n",
       "  - sentence : \"Our friends won\\'t buy this analysis, let alone the next one we propose.\"\n",
       "  ▿ isAcceptable : Optional<Bool>\n",
       "    - some : true\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ Example\n",
       "  - id : \"0\"\n",
       "  - sentence : \"Bill whistled past the house.\"\n",
       "  - isAcceptable : nil\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.testExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ Example\n",
       "  - id : \"gj04\"\n",
       "  - sentence : \"The sailors rode the breeze clear of the rocks.\"\n",
       "  ▿ isAcceptable : Optional<Bool>\n",
       "    - some : true\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.devExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "  Training loss: 1.0054466\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "execution_count": 10,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "Current stack trace:",
      "\tframe #24: 0x00007f2379342a97 libjupyterInstalledPackages.so`AD__$s10TextModels18MultiHeadAttentionV14callAsFunctiony10TensorFlow0I0VySfGAA0E5InputVySfGF__pullback_src_0_wrt_0_1 at Attention.swift:207:22 [opt]",
      "\tframe #25: 0x00007f2379349bdc libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels18MultiHeadAttentionV14callAsFunctiony10TensorFlow0I0VySfGAA0E5InputVySfGF__pullback_src_0_wrt_0_1 at <compiler-generated>:0 [opt]",
      "\tframe #26: 0x00007f23793c99a0 libjupyterInstalledPackages.so`AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at TransformerBERT.swift:298:31 [opt]",
      "\tframe #27: 0x00007f23793d0a2c libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at <compiler-generated>:0 [opt]",
      "\tframe #28: 0x00007f237936815f libjupyterInstalledPackages.so`AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERT.swift:340:60 [opt]",
      "\tframe #29: 0x00007f237937178f libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #30: 0x00007f23793823cc libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #31: 0x00007f23793823c9 libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #32: 0x00007f2379380335 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #33: 0x00007f2379380326 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERTClassifier.swift:40 [opt]",
      "\tframe #34: 0x00007f2379382486 libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #38: 0x00007f237d47cc05 $__lldb_expr108`partial apply for AD__$s15__lldb_expr_10710TensorFlow0C0VySfG10TextModels14BERTClassifierVcfU0___pullback_src_0_wrt_0 [inlined]  at <Cell 10>:24",
      "\tframe #47: 0x00007f237d477f8f $__lldb_expr108`main at <Cell 10>:23:33"
     ]
    }
   ],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)\n",
    "\n",
    "print(\"Training BERT for the Language2Label task!\")\n",
    "for epoch in 1...3 {\n",
    "    print(\"[Epoch \\(epoch)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    var trainingDataIterator = dataset.trainDataIterator\n",
    "\n",
    "    while let batch = withDevice(.cpu, perform: { trainingDataIterator.next() }) {\n",
    "        let (documents, labels) = (batch.inputs, Tensor<Float>(batch.labels!))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return sigmoidCrossEntropy(\n",
    "                logits: logits.squeezingShape(at: -1),\n",
    "                labels: labels,\n",
    "                reduction: { $0.mean() })\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var devDataIterator = dataset.devDataIterator\n",
    "    var devPredictedLabels = [Bool]()\n",
    "    var devGroundTruth = [Bool]()\n",
    "    while let batch = withDevice(.cpu, perform: { devDataIterator.next() }) {\n",
    "        let (documents, labels) = (batch.inputs, batch.labels!)\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = sigmoidCrossEntropy(\n",
    "            logits: logits.squeezingShape(at: -1),\n",
    "            labels: Tensor<Float>(labels),\n",
    "            reduction: { $0.mean() }\n",
    "        )\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let predictedLabels = sigmoid(logits.squeezingShape(at: -1)) .>= 0.5\n",
    "        devPredictedLabels.append(contentsOf: predictedLabels.scalars)\n",
    "        devGroundTruth.append(contentsOf: labels.scalars.map { $0 == 1 })\n",
    "    }\n",
    "\n",
    "    let mcc = matthewsCorrelationCoefficient(\n",
    "        predictions: devPredictedLabels,\n",
    "        groundTruth: devGroundTruth)\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "          MCC: \\(mcc)\n",
    "          Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
