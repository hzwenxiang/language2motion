{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpebh7jqf8/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: \n",
    "// + load csv\n",
    "// + split into train/dev\n",
    "// + convert to [Example]\n",
    "// + get sorted labels\n",
    "// + integrate with Language2Label\n",
    "// - Language2Label code cleanups\n",
    "// - extraxt Language2Label codes\n",
    "// * train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pd  = Python.import(\"pandas\")\n",
    "let model_selection  = Python.import(\"sklearn.model_selection\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Language2Label {\n",
    "//     public let directoryURL: URL\n",
    "    public let trainExamples: [Example]\n",
    "    public let devExamples: [Example]\n",
    "    public let maxSequenceLength: Int\n",
    "    public let batchSize: Int\n",
    "    public let labels: [String]\n",
    "\n",
    "    public typealias ExampleIterator = IndexingIterator<[Example]>\n",
    "    public typealias TrainDataIterator = PrefetchIterator<\n",
    "        GroupedIterator<MapIterator<ExampleIterator, DataBatch>>\n",
    "    >\n",
    "    public typealias DevDataIterator = GroupedIterator<MapIterator<ExampleIterator, DataBatch>>\n",
    "    public typealias TestDataIterator = DevDataIterator\n",
    "\n",
    "    public var trainDataIterator: TrainDataIterator\n",
    "    public var devDataIterator: DevDataIterator\n",
    "}\n",
    "\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "// Data\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "\n",
    "extension Language2Label {\n",
    "    /// Language2Label example.\n",
    "    public  struct Example {\n",
    "        public typealias LabelTuple = (idx: Int, label: String)\n",
    "\n",
    "        public let id: String\n",
    "        public let text: String\n",
    "        public let label: LabelTuple?\n",
    "\n",
    "        public init(id: String, text: String, label: LabelTuple?) {\n",
    "            self.id = id\n",
    "            self.text = text\n",
    "            self.label = label\n",
    "        }\n",
    "    }\n",
    "    /// Language2Label data batch.\n",
    "    public struct DataBatch: KeyPathIterable {\n",
    "        public var inputs: TextBatch  // TODO: !!! Mutable in order to allow for batching.\n",
    "        public var labels: Tensor<Int32>?  // TODO: !!! Mutable in order to allow for batching.\n",
    "\n",
    "        public init(inputs: TextBatch, labels: Tensor<Int32>?) {\n",
    "            self.inputs = inputs\n",
    "            self.labels = labels\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// URL pointing to the downloadable ZIP file that contains the CoLA dataset.\n",
    "//     private static let url: URL = URL(\n",
    "//         string: String(\n",
    "//             \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/\"\n",
    "//                 + \"o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\"))!\n",
    "\n",
    "    internal enum FileType: String {\n",
    "        case train = \"train\"\n",
    "        case dev = \"dev\"\n",
    "        case test = \"test\" // TODO: kill\n",
    "    }\n",
    "//     internal static func load(fromFile fileURL: URL, fileType: FileType) throws -> [Example] {\n",
    "//         let lines = try parse(tsvFileAt: fileURL)\n",
    "\n",
    "//         if fileType == .test {\n",
    "//             // The test data file has a header.\n",
    "//             return lines.dropFirst().enumerated().map { (i, lineParts) in\n",
    "//                 Example(id: lineParts[0], sentence: lineParts[1], isAcceptable: nil)\n",
    "//             }\n",
    "//         }\n",
    "\n",
    "//         return lines.enumerated().map { (i, lineParts) in\n",
    "//             Example(id: lineParts[0], sentence: lineParts[3], isAcceptable: lineParts[1] == \"1\")\n",
    "//         }\n",
    "//     }\n",
    "    \n",
    "    static func Df2Example(df: PythonObject, labels: [String]) -> [Example] {\n",
    "        return Python.list(df.iterrows()).map {\n",
    "            (rowObj: PythonObject) -> Example in \n",
    "            let row = rowObj.tuple2.1\n",
    "            let sample_id: String = \"\\(row.sample_id)\" // Int to String\n",
    "            let text: String = String(row.text)!\n",
    "            let labelStr: String? = String(row.label)\n",
    "            let label: Example.LabelTuple? = Example.LabelTuple(idx: labels.index(of: labelStr!)!, label: labelStr!)\n",
    "            return Example(id: sample_id, text: text, label: label)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// internal func parse(tsvFileAt fileURL: URL) throws -> [[String]] {\n",
    "//     try Data(contentsOf: fileURL).withUnsafeBytes {\n",
    "//         $0.split(separator: UInt8(ascii: \"\\n\")).map {\n",
    "//             $0.split(separator: UInt8(ascii: \"\\t\"), omittingEmptySubsequences: false)\n",
    "//                 .map { String(decoding: UnsafeRawBufferPointer(rebasing: $0), as: UTF8.self) }\n",
    "//         }\n",
    "//     }\n",
    "// }\n",
    "\n",
    "extension Language2Label {\n",
    "    public init(\n",
    "        exampleMap: @escaping (Example) -> DataBatch,\n",
    "        taskDirectoryURL: URL,\n",
    "        maxSequenceLength: Int,\n",
    "        batchSize: Int,\n",
    "        dropRemainder: Bool\n",
    "    ) throws {\n",
    "//         self.directoryURL = taskDirectoryURL.appendingPathComponent(\"CoLA\")\n",
    "//         let dataURL = directoryURL.appendingPathComponent(\"data\")\n",
    "//         let compressedDataURL = dataURL.appendingPathComponent(\"downloaded-data.zip\")\n",
    "\n",
    "//         // Download the data, if necessary.\n",
    "//         try download(from: Language2Label.url, to: compressedDataURL)\n",
    "\n",
    "//         // Extract the data, if necessary.\n",
    "//         let extractedDirectoryURL = compressedDataURL.deletingPathExtension()\n",
    "//         if !FileManager.default.fileExists(atPath: extractedDirectoryURL.path) {\n",
    "//             try extract(zipFileAt: compressedDataURL, to: extractedDirectoryURL)\n",
    "//         }\n",
    "\n",
    "//         #if false\n",
    "//             // FIXME: Need to generalize `DatasetUtilities.downloadResource` to accept\n",
    "//             // arbitrary full URLs instead of constructing full URL from filename and\n",
    "//             // file extension.\n",
    "//             DatasetUtilities.downloadResource(\n",
    "//                 filename: \"\\(subDirectory)\", fileExtension: \"zip\",\n",
    "//                 remoteRoot: url.deletingLastPathComponent(),\n",
    "//                 localStorageDirectory: directory)\n",
    "//         #endif\n",
    "\n",
    "        let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels.csv\")\n",
    "        let df = pd.read_csv(dsURL.path)\n",
    "        labels = df.label.unique().sorted().map {String($0)!}\n",
    "        let (train_df, test_df) = model_selection.train_test_split(df, test_size: 0.2).tuple2\n",
    "        \n",
    "        trainExamples = Language2Label.Df2Example(df: train_df, labels: labels)\n",
    "        devExamples = Language2Label.Df2Example(df: test_df, labels: labels)        \n",
    "\n",
    "        self.maxSequenceLength = maxSequenceLength\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        // Create the data iterators used for training and evaluating.\n",
    "        self.trainDataIterator = trainExamples.shuffled().makeIterator()  // TODO: [RNG] Seed support.\n",
    "            .map(exampleMap)\n",
    "            .grouped(\n",
    "                keyFn: { _ in 0 },\n",
    "                sizeFn: { _ in batchSize / maxSequenceLength },\n",
    "                reduceFn: {\n",
    "                    DataBatch(\n",
    "                        inputs: padAndBatch(\n",
    "                            textBatches: $0.map { $0.inputs }, maxLength: maxSequenceLength),\n",
    "                        labels: Tensor.batch($0.map { $0.labels! }))\n",
    "                },\n",
    "                dropRemainder: dropRemainder\n",
    "            )\n",
    "            .prefetched(count: 2)\n",
    "        self.devDataIterator = devExamples.makeIterator()\n",
    "            .map(exampleMap)\n",
    "            .grouped(\n",
    "                keyFn: { _ in 0 },\n",
    "                sizeFn: { _ in batchSize / maxSequenceLength },\n",
    "                reduceFn: {\n",
    "                    DataBatch(\n",
    "                        inputs: padAndBatch(\n",
    "                            textBatches: $0.map { $0.inputs }, maxLength: maxSequenceLength),\n",
    "                        labels: Tensor.batch($0.map { $0.labels! }))\n",
    "                },\n",
    "                dropRemainder: dropRemainder\n",
    "            )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "// Regarding the batch size, note that the way batching is performed currently is that we bucket\n",
    "// input sequences based on their length (e.g., first bucket contains sequences of length 1 to 10,\n",
    "// second 11 to 20, etc.). We then keep processing examples in the input data pipeline until a\n",
    "// bucket contains enough sequences to form a batch. The batch size specified in the task\n",
    "// constructor specifies the *total number of tokens in the batch* and not the total number of\n",
    "// sequences. So, if the batch size is set to 1024, the first bucket (i.e., lengths 1 to 10)\n",
    "// will need 1024 / 10 = 102 examples to form a batch (every sentence in the bucket is padded\n",
    "// to the max length of the bucket). This kind of bucketing is common practice with NLP models and\n",
    "// it is done to improve memory usage and computational efficiency when dealing with sequences of\n",
    "// varied lengths. Note that this is not used in the original BERT implementation released by\n",
    "// Google and so the batch size setting here is expected to differ from that one.\n",
    "let maxSequenceLength = 128\n",
    "let batchSize = 1024\n",
    "\n",
    "// Create a function that converts examples to data batches.\n",
    "let exampleMapFn: (Language2Label.Example) -> Language2Label.DataBatch = { example -> Language2Label.DataBatch in\n",
    "    let textBatch = bertClassifier.bert.preprocess(\n",
    "        sequences: [example.text],\n",
    "        maxSequenceLength: maxSequenceLength)\n",
    "    return Language2Label.DataBatch( // TODO: get label idx\n",
    "        inputs: textBatch, labels: example.label.map { (label: Language2Label.Example.LabelTuple) in Tensor( Int32(label.idx)) })\n",
    "}\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "    exampleMap: exampleMapFn,\n",
    "    taskDirectoryURL: workspaceURL,\n",
    "    maxSequenceLength: maxSequenceLength,\n",
    "    batchSize: batchSize,\n",
    "    dropRemainder: true)\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ Example\n",
       "  - id : \"1451\"\n",
       "  - text : \"A person lifts up his left arm, bent at the elbow, then moves his right arm across his chest, hand just above the elbow of the raised left arm.\"\n",
       "  ▿ label : Optional<(idx: Int, label: String)>\n",
       "    ▿ some : 2 elements\n",
       "      - idx : 1\n",
       "      - label : \"Performing motions with hands\"\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ Example\n",
       "  - id : \"1982\"\n",
       "  - text : \"A person walks a quarter circle counter clockwise with 4 steps.\"\n",
       "  ▿ label : Optional<(idx: Int, label: String)>\n",
       "    ▿ some : 2 elements\n",
       "      - idx : 2\n",
       "      - label : \"Walking and turning\"\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.devExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "  Training loss: 1.5286338\n",
      "  Training loss: 1.6280258\n",
      "  Training loss: 1.7589718\n",
      "  Training loss: 1.7574695\n",
      "  Training loss: 1.8055675\n",
      "  Training loss: 1.798664\n",
      "  Training loss: 1.8284141\n",
      "  Training loss: 1.8270429\n",
      "  Training loss: 1.8305086\n",
      "  Training loss: 1.7910993\n",
      "  Training loss: 1.7708272\n",
      "  Training loss: 1.7626184\n",
      "  Training loss: 1.7480073\n",
      "  Training loss: 1.7085618\n",
      "  Training loss: 1.679118\n",
      "  Training loss: 1.6711938\n",
      "  Training loss: 1.6812017\n",
      "  Training loss: 1.6623082\n",
      "  Training loss: 1.6307421\n",
      "  Training loss: 1.6271813\n",
      "  Training loss: 1.6146624\n",
      "  Training loss: 1.6112064\n",
      "  Training loss: 1.5804316\n",
      "  Training loss: 1.5612737\n",
      "  Training loss: 1.5464345\n",
      "  Training loss: 1.523027\n",
      "  Training loss: 1.5425303\n",
      "  Training loss: 1.5300928\n",
      "  Training loss: 1.5282929\n",
      "  Training loss: 1.534577\n",
      "  Training loss: 1.5237814\n",
      "  Training loss: 1.5066097\n",
      "  Training loss: 1.4923956\n",
      "  Training loss: 1.5102407\n",
      "  Training loss: 1.4931664\n",
      "  Training loss: 1.476955\n",
      "  Training loss: 1.4717367\n",
      "  Training loss: 1.469575\n",
      "  Training loss: 1.4680288\n",
      "  Training loss: 1.46195\n",
      "  Training loss: 1.4595644\n",
      "  Training loss: 1.4574214\n",
      "  Training loss: 1.4453045\n",
      "  Training loss: 1.4464257\n",
      "  Training loss: 1.4472861\n",
      "  Training loss: 1.4405302\n",
      "  Training loss: 1.4280812\n",
      "  Training loss: 1.4182435\n",
      "  Training loss: 1.4143708\n",
      "  Training loss: 1.4078158\n",
      "  Training loss: 1.4012065\n",
      "  Training loss: 1.3961686\n",
      "  Training loss: 1.3879311\n",
      "  Training loss: 1.3819153\n",
      "  Training loss: 1.3768135\n",
      "  Training loss: 1.3653166\n",
      "  Training loss: 1.3626797\n",
      "  Training loss: 1.3643384\n",
      "  Training loss: 1.3598357\n",
      "  Training loss: 1.3565427\n",
      "  Training loss: 1.3572669\n",
      "  Training loss: 1.3574573\n",
      "  Training loss: 1.3589888\n",
      "  Training loss: 1.3585302\n",
      "  Training loss: 1.3596941\n",
      "  Training loss: 1.3549716\n",
      "  Training loss: 1.350769\n",
      "  Training loss: 1.3460071\n",
      "  Training loss: 1.335315\n",
      "  Training loss: 1.3257028\n",
      "  Training loss: 1.3190401\n",
      "  Training loss: 1.3163903\n",
      "  Training loss: 1.3121107\n",
      "  Training loss: 1.3120255\n",
      "  Training loss: 1.3139547\n",
      "  Training loss: 1.3099802\n",
      "  Training loss: 1.3055114\n",
      "  Training loss: 1.3019912\n",
      "  Training loss: 1.3054675\n",
      "  Training loss: 1.3010924\n",
      "  Training loss: 1.2986537\n",
      "  Training loss: 1.2961389\n",
      "  Training loss: 1.2923317\n",
      "  Training loss: 1.2897067\n",
      "  Training loss: 1.2886859\n",
      "  Training loss: 1.2878528\n",
      "  Training loss: 1.2842423\n",
      "  Training loss: 1.2825559\n",
      "  Training loss: 1.2787559\n",
      "  Training loss: 1.2773336\n",
      "  Training loss: 1.2728256\n",
      "  Training loss: 1.268108\n",
      "  Training loss: 1.2645948\n",
      "  Training loss: 1.2646381\n",
      "  Training loss: 1.2626796\n",
      "  Training loss: 1.2573789\n",
      "  Training loss: 1.2574291\n",
      "  Training loss: 1.2578548\n",
      "  Training loss: 1.2606219\n",
      "  Training loss: 1.2588294\n",
      "  Training loss: 1.2563767\n",
      "  Training loss: 1.2539654\n",
      "  Training loss: 1.2547289\n",
      "  Training loss: 1.258179\n",
      "  Training loss: 1.2533497\n",
      "  Training loss: 1.2504119\n",
      "  Training loss: 1.250742\n",
      "  Training loss: 1.2519141\n",
      "  Training loss: 1.2490132\n",
      "  Training loss: 1.2485107\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "execution_count": 11,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "Current stack trace:",
      "\tframe #26: 0x00007fc89310e426 libjupyterInstalledPackages.so`AD__$s10TextModels18MultiHeadAttentionV14callAsFunctiony10TensorFlow0I0VySfGAA0E5InputVySfGF__pullback_src_0_wrt_0_1 at Attention.swift:192:31 [opt]",
      "\tframe #27: 0x00007fc893114bdc libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels18MultiHeadAttentionV14callAsFunctiony10TensorFlow0I0VySfGAA0E5InputVySfGF__pullback_src_0_wrt_0_1 at <compiler-generated>:0 [opt]",
      "\tframe #28: 0x00007fc893194970 libjupyterInstalledPackages.so`AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at TransformerBERT.swift:298:31 [opt]",
      "\tframe #29: 0x00007fc89319b9fc libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at <compiler-generated>:0 [opt]",
      "\tframe #30: 0x00007fc893133159 libjupyterInstalledPackages.so`AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERT.swift:340:60 [opt]",
      "\tframe #31: 0x00007fc89313c75f libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #32: 0x00007fc89314d39c libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #33: 0x00007fc89314d399 libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #34: 0x00007fc89314b305 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #35: 0x00007fc89314b2f6 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERTClassifier.swift:40 [opt]",
      "\tframe #36: 0x00007fc89314d456 libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #40: 0x00007fc83e372619 $__lldb_expr102`partial apply for AD__$s15__lldb_expr_10110TensorFlow0C0VySfG10TextModels14BERTClassifierVcfU0___pullback_src_0_wrt_0 [inlined]  at <Cell 11>:12",
      "\tframe #49: 0x00007fc83e36de97 $__lldb_expr102`main at <Cell 11>:11:33"
     ]
    }
   ],
   "source": [
    "print(\"Training BERT for the Language2Label task!\")\n",
    "for epoch in 1...3 {\n",
    "    print(\"[Epoch \\(epoch)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    var trainingDataIterator = dataset.trainDataIterator\n",
    "    \n",
    "    while let batch = withDevice(.cpu, perform: { trainingDataIterator.next() }) {\n",
    "        let (documents, labels) = (batch.inputs, Tensor<Int32>(batch.labels!))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "                                                                    \n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var devDataIterator = dataset.devDataIterator\n",
    "    var devPredictedLabels = [Bool]()\n",
    "    var devGroundTruth = [Bool]()\n",
    "    while let batch = withDevice(.cpu, perform: { devDataIterator.next() }) {\n",
    "        print(\"test batch\")\n",
    "        let (documents, labels) = (batch.inputs, batch.labels!)\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = sigmoidCrossEntropy(\n",
    "            logits: logits.squeezingShape(at: -1),\n",
    "            labels: Tensor<Float>(labels),\n",
    "            reduction: { $0.mean() }\n",
    "        )\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let predictedLabels = sigmoid(logits.squeezingShape(at: -1)) .>= 0.5\n",
    "        devPredictedLabels.append(contentsOf: predictedLabels.scalars)\n",
    "        devGroundTruth.append(contentsOf: labels.scalars.map { $0 == 1 })\n",
    "    }\n",
    "    print(4)\n",
    "\n",
    "    let mcc = matthewsCorrelationCoefficient(\n",
    "        predictions: devPredictedLabels,\n",
    "        groundTruth: devGroundTruth)\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "          MCC: \\(mcc)\n",
    "          Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
