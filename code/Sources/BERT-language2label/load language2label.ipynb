{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmp6v1o5k6u/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: \n",
    "// + load csv\n",
    "// + split into train/dev\n",
    "// + convert to [Example]\n",
    "// + get sorted labels\n",
    "// + integrate with Language2Label\n",
    "// + Language2Label code cleanups\n",
    "// - extract Language2Label codes\n",
    "// + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pd  = Python.import(\"pandas\")\n",
    "let model_selection  = Python.import(\"sklearn.model_selection\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// A `TextBatch` with the corresponding labels.\n",
    "public typealias LabeledTextBatch = (data: TextBatch, label: Tensor<Int32>)\n",
    "\n",
    "\n",
    "/// Language2Label example.\n",
    "public struct Language2LabelExample {\n",
    "    public typealias LabelTuple = (idx: Int, label: String)\n",
    "\n",
    "    public let id: String\n",
    "    public let text: String\n",
    "    public let label: LabelTuple?\n",
    "\n",
    "    public init(id: String, text: String, label: LabelTuple?) {\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "public struct Language2Label <Entropy: RandomNumberGenerator> {\n",
    "    public typealias Samples = LazyMapSequence<[Language2LabelExample], LabeledTextBatch>\n",
    "    \n",
    "    /// The training texts.\n",
    "    public let trainingExamples: Samples\n",
    "    /// The validation texts.\n",
    "    public let validationExamples: Samples\n",
    "\n",
    "    /// The sequence length to which every sentence will be padded.\n",
    "    public let maxSequenceLength: Int\n",
    "    public let batchSize: Int\n",
    "    public let labels: [String]\n",
    "\n",
    "    /// The type of the collection of batches.\n",
    "    public typealias Batches = Slices<Sampling<Samples, ArraySlice<Int>>>\n",
    "    /// The type of the training sequence of epochs.\n",
    "    public typealias TrainEpochs = LazyMapSequence<TrainingEpochs<Samples, Entropy>, \n",
    "        LazyMapSequence<Batches, LabeledTextBatch>>\n",
    "    /// The sequence of training data (epochs of batches).\n",
    "    public var trainingEpochs: TrainEpochs\n",
    "    /// The validation batches.\n",
    "    public var validationBatches: LazyMapSequence<Slices<Samples>, LabeledTextBatch>    \n",
    "}\n",
    "\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "// Data\n",
    "//===-----------------------------------------------------------------------------------------===//\n",
    "\n",
    "extension Language2Label {\n",
    "\n",
    "    internal enum FileType: String {\n",
    "        case train = \"train\"\n",
    "        case dev = \"dev\"\n",
    "    }\n",
    "    \n",
    "    static func Df2Example(df: PythonObject, labels: [String]) -> [Language2LabelExample] {\n",
    "        return Python.list(df.iterrows()).map {\n",
    "            (rowObj: PythonObject) -> Language2LabelExample in \n",
    "            let row = rowObj.tuple2.1\n",
    "            let sample_id: String = \"\\(row.sample_id)\" // Int to String\n",
    "            let text: String = String(row.text)!\n",
    "            let labelStr: String? = String(row.label)\n",
    "            let label: Language2LabelExample.LabelTuple? = Language2LabelExample.LabelTuple(idx: labels.firstIndex(of: labelStr!)!, label: labelStr!)\n",
    "            return Language2LabelExample(id: sample_id, text: text, label: label)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "extension Language2Label {\n",
    "  /// Creates an instance in `taskDirectoryURL` with batches of size `batchSize`\n",
    "  /// by `maximumSequenceLength`.\n",
    "  ///\n",
    "  /// - Parameters:\n",
    "  ///   - entropy: a source of randomness used to shuffle sample ordering. It\n",
    "  ///     will be stored in `self`, so if it is only pseudorandom and has value\n",
    "  ///     semantics, the sequence of epochs is determinstic and not dependent on\n",
    "  ///     other operations.\n",
    "  ///   - exampleMap: a transform that processes `Example` in `LabeledTextBatch`.\n",
    "  public init(\n",
    "    taskDirectoryURL: URL,\n",
    "    maxSequenceLength: Int,\n",
    "    batchSize: Int,\n",
    "    entropy: Entropy,\n",
    "    exampleMap: @escaping (Language2LabelExample) -> LabeledTextBatch\n",
    "  ) throws {\n",
    "    // Load the data file.\n",
    "        let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels.csv\")\n",
    "        let df = pd.read_csv(dsURL.path)\n",
    "        labels = df.label.unique().sorted().map {String($0)!}\n",
    "        let (train_df, test_df) = model_selection.train_test_split(df, test_size: 0.2).tuple2\n",
    "        \n",
    "        trainingExamples = Language2Label.Df2Example(df: train_df, labels: labels).lazy.map(exampleMap)\n",
    "        validationExamples = Language2Label.Df2Example(df: test_df, labels: labels).lazy.map(exampleMap)\n",
    "      \n",
    "      \n",
    "    self.maxSequenceLength = maxSequenceLength\n",
    "    self.batchSize = batchSize\n",
    "\n",
    "    // Create the training sequence of epochs.\n",
    "    trainingEpochs = TrainingEpochs(\n",
    "      samples: trainingExamples, batchSize: batchSize / maxSequenceLength, entropy: entropy\n",
    "    ).lazy.map { (batches: Batches) -> LazyMapSequence<Batches, LabeledTextBatch> in\n",
    "      batches.lazy.map{ \n",
    "        (\n",
    "          data: $0.map(\\.data).paddedAndCollated(to: maxSequenceLength),\n",
    "          label: Tensor($0.map(\\.label))\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    // Create the validation collection of batches.\n",
    "    validationBatches = validationExamples.inBatches(of: batchSize / maxSequenceLength).lazy.map{ \n",
    "      (\n",
    "        data: $0.map(\\.data).paddedAndCollated(to: maxSequenceLength),\n",
    "        label: Tensor($0.map(\\.label))\n",
    "      )\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "extension Language2Label where Entropy == SystemRandomNumberGenerator {\n",
    "  /// Creates an instance in `taskDirectoryURL` with batches of size `batchSize`\n",
    "  /// by `maximumSequenceLength`.\n",
    "  ///\n",
    "  /// - Parameter exampleMap: a transform that processes `Example` in `LabeledTextBatch`.\n",
    "  public init(\n",
    "    taskDirectoryURL: URL,\n",
    "    maxSequenceLength: Int,\n",
    "    batchSize: Int,\n",
    "    exampleMap: @escaping (Language2LabelExample) -> LabeledTextBatch\n",
    "  ) throws {\n",
    "    try self.init(\n",
    "      taskDirectoryURL: taskDirectoryURL,\n",
    "      maxSequenceLength: maxSequenceLength,\n",
    "      batchSize: batchSize,\n",
    "      entropy: SystemRandomNumberGenerator(),\n",
    "      exampleMap: exampleMap\n",
    "    )\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "// Regarding the batch size, note that the way batching is performed currently is that we bucket\n",
    "// input sequences based on their length (e.g., first bucket contains sequences of length 1 to 10,\n",
    "// second 11 to 20, etc.). We then keep processing examples in the input data pipeline until a\n",
    "// bucket contains enough sequences to form a batch. The batch size specified in the task\n",
    "// constructor specifies the *total number of tokens in the batch* and not the total number of\n",
    "// sequences. So, if the batch size is set to 1024, the first bucket (i.e., lengths 1 to 10)\n",
    "// will need 1024 / 10 = 102 examples to form a batch (every sentence in the bucket is padded\n",
    "// to the max length of the bucket). This kind of bucketing is common practice with NLP models and\n",
    "// it is done to improve memory usage and computational efficiency when dealing with sequences of\n",
    "// varied lengths. Note that this is not used in the original BERT implementation released by\n",
    "// Google and so the batch size setting here is expected to differ from that one.\n",
    "let maxSequenceLength = 128\n",
    "let batchSize = 1024\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "  taskDirectoryURL: workspaceURL,\n",
    "  maxSequenceLength: maxSequenceLength,\n",
    "  batchSize: batchSize,\n",
    "  entropy: SystemRandomNumberGenerator()\n",
    ") { (example: Language2LabelExample) -> LabeledTextBatch in\n",
    "  let textBatch = bertClassifier.bert.preprocess(\n",
    "    sequences: [example.text],\n",
    "    maxSequenceLength: maxSequenceLength)\n",
    "   return (data: textBatch, \n",
    "           label: example.label.map { \n",
    "               (label: Language2LabelExample.LabelTuple) in Tensor(Int32(label.idx))\n",
    "           }!\n",
    "          )\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[  101,  1037,  2711, 13529,  2015,  2006,  1996,  2723,   102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 0\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[ 101, 1037, 2529, 7365, 2830, 1998, 3632, 2039, 1996, 5108, 1012,  102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 0\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.validationExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "  Training loss: 1.9629791\n",
      "  Training loss: 1.7798477\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "execution_count": 11,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "Current stack trace:",
      "\tframe #27: 0x00007fa1709c6409 libjupyterInstalledPackages.so`AD__orig_$s10TensorFlow7DropoutV14callAsFunctionyAA0A0VyxGAGF_$s10TensorFlow0A0VySfGAA7DropoutVySfGADxq_AA18EmptyTangentVectorVr0_lyA2DIseggod_Ieggyoo_Adg3DIeggo_Ieggyoo_TR_src_0_wrt_0_vjp_subset_parameters_thunk at <compiler-generated>:0 [opt]",
      "\tframe #28: 0x00007fa170a7467f libjupyterInstalledPackages.so`AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__vjp_src_0_wrt_0_1(input=<unavailable>, self=TextModels.TransformerEncoderLayer @ 0x00007fffcecae058) at TransformerBERT.swift:313:31 [opt]",
      "\tframe #29: 0x00007fa1709f19b9 libjupyterInstalledPackages.so`AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__vjp_src_0_wrt_1(input=<unavailable>, self=<unavailable>) at BERT.swift:340:61 [opt]",
      "\tframe #30: 0x00007fa170a15224 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__vjp_src_0_wrt_1(input=<unavailable>, self=TextModels.BERTClassifier @ 0x00007fffcecae930) at BERTClassifier.swift:40:15 [opt]",
      "\tframe #31: 0x00007fa120011e1d $__lldb_expr102`partial apply for AD__$s15__lldb_expr_10110TensorFlow0C0VySfG10TextModels14BERTClassifierVcfU___vjp_src_0_wrt_0 at <Cell 11>:12:31",
      "\tframe #37: 0x00007fa12000e20d $__lldb_expr102`main at <Cell 11>:11:33"
     ]
    }
   ],
   "source": [
    "print(\"Training BERT for the Language2Label task!\")\n",
    "\n",
    "for (epoch, epochBatches) in dataset.trainingEpochs.prefix(3).enumerated() {\n",
    "    print(\"[Epoch \\(epoch + 1)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "\n",
    "    for batch in epochBatches {\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var devPredictedLabels = [Bool]()\n",
    "    var devGroundTruth = [Bool]()\n",
    "    for batch in dataset.validationBatches {\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let predictedLabels = sigmoid(logits.squeezingShape(at: -1)) .>= 0.5\n",
    "        devPredictedLabels.append(contentsOf: predictedLabels.scalars)\n",
    "        devGroundTruth.append(contentsOf: labels.scalars.map { $0 == 1 })\n",
    "    }\n",
    "\n",
    "    let mcc = matthewsCorrelationCoefficient(\n",
    "        predictions: devPredictedLabels,\n",
    "        groundTruth: devGroundTruth)\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "          MCC: \\(mcc)\n",
    "          Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
