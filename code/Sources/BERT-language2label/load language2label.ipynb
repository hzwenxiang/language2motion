{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpym3mhhjj/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: \n",
    "// + configure dataset path outside of Language2Label\n",
    "// + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"/notebooks/language2motion.gt/code/Sources/BERT-language2label/Language2Label.swift\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "let maxSequenceLength = 50\n",
    "let batchSize = 2048\n",
    "\n",
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v2_manual_labels.csv\")\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "  datasetURL: dsURL,\n",
    "  maxSequenceLength: maxSequenceLength,\n",
    "  batchSize: batchSize,\n",
    "  entropy: SystemRandomNumberGenerator()\n",
    ") { (example: Language2LabelExample) -> LabeledTextBatch in\n",
    "  let textBatch = bertClassifier.bert.preprocess(\n",
    "    sequences: [example.text],\n",
    "    maxSequenceLength: maxSequenceLength)\n",
    "   return (data: textBatch, \n",
    "           label: example.label.map { \n",
    "               (label: Language2LabelExample.LabelTuple) in Tensor(Int32(label.idx))\n",
    "           }!\n",
    "          )\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[ 101, 1037, 2711, 7365, 1999, 1037, 4418, 1012,  102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 4\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[ 101, 1037, 2711, 3084, 1037, 2502, 3357, 2000, 1996, 2187, 1010, 2066, 2108, 3724, 1012,  102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 0\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.validationExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "epochBatches.count: 60\n",
      "  Training loss: 1.4558853\n",
      "  Training loss: 1.5414033\n",
      "  Training loss: 1.564405\n",
      "  Training loss: 1.5679815\n",
      "  Training loss: 1.5503405\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BERT for the Language2Label task!\")\n",
    "\n",
    "for (epoch, epochBatches) in dataset.trainingEpochs.prefix(3).enumerated() {\n",
    "    print(\"[Epoch \\(epoch + 1)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    print(\"epochBatches.count: \\(epochBatches.count)\")\n",
    "\n",
    "    for batch in epochBatches {\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print(\"dataset.validationBatches.count: \\(dataset.validationBatches.count)\")\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var correctGuessCount = 0\n",
    "    var totalGuessCount = 0\n",
    "\n",
    "    for batch in dataset.validationBatches {\n",
    "        let valBatchSize = batch.data.tokenIds.shape[0]\n",
    "\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
    "\n",
    "        correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())\n",
    "        totalGuessCount += valBatchSize\n",
    "    }\n",
    "    \n",
    "    let accuracy = Float(correctGuessCount) / Float(totalGuessCount)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Accuracy: \\(correctGuessCount)/\\(totalGuessCount) (\\(accuracy)) \\\n",
    "        Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Prediction {\n",
    "    public let classIdx: Int\n",
    "    public let className: String\n",
    "    public let probability: Float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: get num_best preds\n",
    "func predict(_ texts: [String], bertClassifier: BERTClassifier) -> [Prediction] {\n",
    "    print(\"predict()\")\n",
    "    print(\"texts: \\(texts.count)\")\n",
    "\n",
    "    let validationExamples = texts.map {\n",
    "        (text) -> TextBatch in\n",
    "        return bertClassifier.bert.preprocess(\n",
    "            sequences: [text],\n",
    "            maxSequenceLength: maxSequenceLength\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(\"validationExamples.count: \\(validationExamples.count)\")\n",
    "\n",
    "    print(\"batchSize: \\(batchSize)\")\n",
    "    print(\"maxSequenceLength: \\(maxSequenceLength)\")\n",
    "    print(\"batchSize / maxSequenceLength: \\(batchSize / maxSequenceLength)\")\n",
    "\n",
    "    let validationBatches = validationExamples.inBatches(of: batchSize / maxSequenceLength).map { \n",
    "        $0.paddedAndCollated(to: maxSequenceLength)\n",
    "    }\n",
    "    print(\"validationBatches: \\(validationBatches.count)\")\n",
    "    var preds: [Prediction] = []\n",
    "    for batch in validationBatches {\n",
    "        print(\"batch\")\n",
    "        let logits = bertClassifier(batch)\n",
    "        let probs = softmax(logits, alongAxis: 1)\n",
    "        let classIdxs = logits.argmax(squeezingAxis: 1)\n",
    "        let batchPreds = (0..<classIdxs.shape[0]).map { \n",
    "            (idx) -> Prediction in\n",
    "            let classIdx: Int = Int(classIdxs[idx].scalar!)\n",
    "            let prob = probs[idx, classIdx].scalar!\n",
    "            return Prediction(classIdx: classIdx, className: dataset.labels[classIdx], probability: prob)\n",
    "        }\n",
    "        preds.append(contentsOf: batchPreds)\n",
    "    }\n",
    "    return preds\n",
    "}\n",
    "\n",
    "let texts = [\n",
    "    \"A person is walking forwards.\", \n",
    "    \"A person walks 4 steps forward.\", \n",
    "    \"A person walks in a circle counter clockwise.\", \n",
    "    \"A person getting done on their knees\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do inference on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v2_manual_labels.csv\")\n",
    "let df = pd.read_csv(dsURL.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let labels = df.label.unique().sorted().map {String($0)!}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let texts2: [String] = Array(df.text.to_list())! // .iloc[0..<2000]\n",
    "texts2.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let preds2 = predict(texts2, bertClassifier: bertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (idx, pred) in preds2.enumerated() {\n",
    "    print(idx, texts2[idx], pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// + save preds\n",
    "// + new notebook\n",
    "// + confusion matrix\n",
    "// + review bad preds\n",
    "// + review ambiguous preds\n",
    "// + create manual labels\n",
    "// * retrain bert\n",
    "// - retrain img2label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## save preds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"pred\"] = PythonObject(preds2.map { $0.className })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"prob\"] = PythonObject(preds2.map { $0.probability })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.to_csv(\"/notebooks/language2motion.gt/data/labels_ds_v1_preds.csv\", index: false)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## max seq len in tokens?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bertClassifier.bert.preprocess(sequences: [\"ala ma kota\"], maxSequenceLength: 200).tokenIds.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let tokenCounts = texts2.map { bertClassifier.bert.preprocess(sequences: [$0], maxSequenceLength: 200).tokenIds.shape[1] }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenCounts.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
