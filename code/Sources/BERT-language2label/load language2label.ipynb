{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpts_beqsx/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: \n",
    "// - configure dataset path outside of Language2Label\n",
    "// + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"/notebooks/language2motion.gt/code/Sources/BERT-language2label/Language2Label.swift\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "let maxSequenceLength = 20\n",
    "let batchSize = 2048\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "  taskDirectoryURL: workspaceURL,\n",
    "  maxSequenceLength: maxSequenceLength,\n",
    "  batchSize: batchSize,\n",
    "  entropy: SystemRandomNumberGenerator()\n",
    ") { (example: Language2LabelExample) -> LabeledTextBatch in\n",
    "  let textBatch = bertClassifier.bert.preprocess(\n",
    "    sequences: [example.text],\n",
    "    maxSequenceLength: maxSequenceLength)\n",
    "   return (data: textBatch, \n",
    "           label: example.label.map { \n",
    "               (label: Language2LabelExample.LabelTuple) in Tensor(Int32(label.idx))\n",
    "           }!\n",
    "          )\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[ 101, 1037, 2529, 2003, 3788, 3407,  102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 4\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[  101,  1037,  2711, 14523,  2830,   102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1]]\n",
       "  - label : 4\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.validationExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "epochBatches.count: 23\n",
      "  Training loss: 1.9004849\n",
      "  Training loss: 1.8883357\n",
      "  Training loss: 1.8536991\n",
      "  Training loss: 1.8264134\n",
      "  Training loss: 1.7747473\n",
      "  Training loss: 1.7418782\n",
      "  Training loss: 1.7116826\n",
      "  Training loss: 1.6916653\n",
      "  Training loss: 1.6637888\n",
      "  Training loss: 1.6384094\n",
      "  Training loss: 1.6082419\n",
      "  Training loss: 1.5870118\n",
      "  Training loss: 1.5518224\n",
      "  Training loss: 1.5203637\n",
      "  Training loss: 1.4909145\n",
      "  Training loss: 1.4588009\n",
      "  Training loss: 1.4305873\n",
      "  Training loss: 1.4030278\n",
      "  Training loss: 1.3693583\n",
      "  Training loss: 1.337987\n",
      "  Training loss: 1.3086315\n",
      "  Training loss: 1.2816018\n",
      "  Training loss: 1.2540916\n",
      "dataset.validationBatches.count: 6\n",
      "Accuracy: 478/603 (0.79270315) Eval loss: 0.60312885\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BERT for the Language2Label task!\")\n",
    "\n",
    "for (epoch, epochBatches) in dataset.trainingEpochs.prefix(1).enumerated() {\n",
    "    print(\"[Epoch \\(epoch + 1)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    print(\"epochBatches.count: \\(epochBatches.count)\")\n",
    "\n",
    "    for batch in epochBatches {\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print(\"dataset.validationBatches.count: \\(dataset.validationBatches.count)\")\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var correctGuessCount = 0\n",
    "    var totalGuessCount = 0\n",
    "\n",
    "    for batch in dataset.validationBatches {\n",
    "        let valBatchSize = batch.data.tokenIds.shape[0]\n",
    "\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
    "\n",
    "        correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())\n",
    "        totalGuessCount += valBatchSize\n",
    "    }\n",
    "    \n",
    "    let accuracy = Float(correctGuessCount) / Float(totalGuessCount)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Accuracy: \\(correctGuessCount)/\\(totalGuessCount) (\\(accuracy)) \\\n",
    "        Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Prediction {\n",
    "    public let classIdx: Int\n",
    "    public let className: String\n",
    "    public let probability: Float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: get num_best preds\n",
    "func predict(_ texts: [String], bertClassifier: BERTClassifier) -> [Prediction] {\n",
    "    print(\"predict()\")\n",
    "    print(\"texts: \\(texts.count)\")\n",
    "\n",
    "//     let textBatch = bertClassifier.bert.preprocess(\n",
    "//     sequences: texts,\n",
    "//     maxSequenceLength: maxSequenceLength)\n",
    "\n",
    "    let validationExamples = texts.map {\n",
    "        (text) -> TextBatch in\n",
    "//         print(text)\n",
    "        return bertClassifier.bert.preprocess(\n",
    "            sequences: [text],\n",
    "            maxSequenceLength: maxSequenceLength\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(\"validationExamples.count: \\(validationExamples.count)\")\n",
    "//     print(\"validationExamples: \\(validationExamples)\")\n",
    "\n",
    "    print(\"batchSize: \\(batchSize)\")\n",
    "    print(\"maxSequenceLength: \\(maxSequenceLength)\")\n",
    "    print(\"batchSize / maxSequenceLength: \\(batchSize / maxSequenceLength)\")\n",
    "\n",
    "    let validationBatches = validationExamples.inBatches(of: batchSize / maxSequenceLength).map { \n",
    "        $0.paddedAndCollated(to: maxSequenceLength)\n",
    "    }\n",
    "    print(\"validationBatches: \\(validationBatches.count)\")\n",
    "    var preds: [Prediction] = []\n",
    "    for batch in validationBatches {\n",
    "        print(\"batch\")\n",
    "        let logits = bertClassifier(batch)\n",
    "//         print(\"logits.shape: \\(logits.shape)\")\n",
    "//         print(logits)\n",
    "        let probs = softmax(logits, alongAxis: 1)\n",
    "//         print(\"probs.shape: \\(probs.shape)\")\n",
    "//         print(probs)\n",
    "        let classIdxs = logits.argmax(squeezingAxis: 1)\n",
    "//         print(classIdxs)\n",
    "        let batchPreds = (0..<classIdxs.shape[0]).map { \n",
    "            (idx) -> Prediction in\n",
    "//             print(\"idx: \\(idx)\")\n",
    "            let classIdx: Int = Int(classIdxs[idx].scalar!)\n",
    "//             print(\"classIdx\", classIdx, type(of: classIdx))\n",
    "            let prob = probs[idx, classIdx].scalar!\n",
    "//             print(\"prob\", prob, type(of: prob))\n",
    "            return Prediction(classIdx: classIdx, className: dataset.labels[classIdx], probability: prob)\n",
    "        }\n",
    "//         print(batchPreds)\n",
    "        preds.append(contentsOf: batchPreds)\n",
    "    }\n",
    "    return preds\n",
    "}\n",
    "\n",
    "let texts = [\n",
    "    \"A person is walking forwards.\", \n",
    "    \"A person walks 4 steps forward.\", \n",
    "    \"A person walks in a circle counter clockwise.\", \n",
    "    \"A person getting done on their knees\"\n",
    "]\n",
    "// let preds = predict(texts, bertClassifier: bertClassifier)\n",
    "\n",
    "// for (idx, pred) in preds.enumerated() {\n",
    "//     print(idx, texts[idx], pred)\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do inference on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v1.csv\")\n",
    "let df = pd.read_csv(dsURL.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 5 elements\n",
       "  - 0 : \"Doing something\"\n",
       "  - 1 : \"Performing motions with hands\"\n",
       "  - 2 : \"Walking and turning\"\n",
       "  - 3 : \"Walking forward few steps\"\n",
       "  - 4 : \"Walking or running\"\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let labels = df.label.unique().sorted().map {String($0)!}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let texts2: [String] = Array(df.text.to_list())! // .iloc[0..<2000]\n",
    "texts2.count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let validationExamples = texts2.map {\n",
    "    (text) -> TextBatch in\n",
    "    print(text)\n",
    "    return bertClassifier.bert.preprocess(\n",
    "        sequences: [text],\n",
    "        maxSequenceLength: maxSequenceLength\n",
    "    )\n",
    "}\n",
    "print(type(of:validationExamples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict()\n",
      "texts: 3012\n",
      "validationExamples.count: 3012\n",
      "batchSize: 2048\n",
      "maxSequenceLength: 20\n",
      "batchSize / maxSequenceLength: 102\n",
      "validationBatches: 30\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n"
     ]
    }
   ],
   "source": [
    "let preds2 = predict(texts2, bertClassifier: bertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, pred) in preds2.enumerated() {\n",
    "    print(idx, texts2[idx], pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
