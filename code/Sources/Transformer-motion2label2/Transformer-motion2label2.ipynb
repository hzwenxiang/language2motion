{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-motion2label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tImageClassificationModels\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmpdtkvuv9u/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets ImageClassificationModels TextModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// + load Motion2Label dataset\n",
    "// + create sliding resnet feature extractor\n",
    "// * create tiny transformer encoder (small hiddenSize, smaller than 768)\n",
    "// + copy relevant sources here\n",
    "// * feed sliced features to transformer\n",
    "// TODO: make it train (with classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import TensorFlow\n",
    "import PythonKit\n",
    "\n",
    "import Batcher\n",
    "import ModelSupport\n",
    "import Datasets\n",
    "import ImageClassificationModels\n",
    "import TextModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MotionData(motionSamples: 494)\n",
      "trainTensorPairs.count = 395\n",
      "testTensorPairs.count = 99\n",
      "dataset.training.count: 198\n",
      "dataset.test.count: 50\n"
     ]
    }
   ],
   "source": [
    "let batchSize = 2\n",
    "let tensorWidth = 60\n",
    "\n",
    "let serializedDatasetURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/motion_dataset.motion_flag.normalized.500.plist\")\n",
    "let labelsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v2.csv\")\n",
    "\n",
    "let dataset = Motion2Label(\n",
    "    batchSize: batchSize, \n",
    "    serializedDatasetURL: serializedDatasetURL,\n",
    "    labelsURL: labelsURL,\n",
    "    tensorWidth: tensorWidth\n",
    ")\n",
    "print(\"dataset.training.count: \\(dataset.training.count)\")\n",
    "print(\"dataset.test.count: \\(dataset.test.count)\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# replace motion flag"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let mfa = dataset.motionData.motionSamples[0].motionFramesArray\n",
    "var t = Tensor(mfa)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let mfIdx = 44"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let mf = t[0..., mfIdx...mfIdx]\n",
    "mf.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mf[0...3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t = sigmoid(t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t[0...3, mfIdx...mfIdx]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t[0..., mfIdx...mfIdx] = mf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t[0...3, mfIdx...mfIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Motion2LabelBatch batch or batch of Motion2LabelSample(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-channel ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "let nOutputs = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var resnet = ResNet(classCount: nOutputs, depth: .resNet18, downsamplingInFirstStage: false, channelCount: 1)\n",
    "// let optimizer = SGD(for: resnet, learningRate: 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding ResNet feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "var batchIterator = dataset.training.sequenced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batch = batchIterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ [2, 60, 45, 1]\n",
       "  ▿ dimensions : 4 elements\n",
       "    - 0 : 2\n",
       "    - 1 : 60\n",
       "    - 2 : 45\n",
       "    - 3 : 1\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let batchTensor = batch!.first\n",
    "batchTensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "let stride = 10\n",
    "let tWidth = stride*2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "func emb(_ batchTensor: Tensor<Float>) -> Tensor<Float> {\n",
    "    var emb1: [Tensor<Float>] = []\n",
    "\n",
    "    for i in 0..<(tensorWidth/stride)-1 {\n",
    "        let start = i*stride\n",
    "        let end = i*stride+tWidth\n",
    "        // print(start, end)\n",
    "        let t1 = batchTensor[0..., start..<end]\n",
    "        let emb2 = resnet(t1)\n",
    "        emb1.append(emb2)\n",
    "    }\n",
    "    let emb3 = Tensor(stacking: emb1, alongAxis: 1)\n",
    "    return emb3\n",
    "}\n",
    "time() {\n",
    "    let emb3 = emb(batchTensor)\n",
    "    print(emb3.shape)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 60, 45, 1]\n",
      "[2, 5, 768]\n",
      "average: 387.7104 ms,   min: 387.7104 ms,   max: 387.7104 ms\n"
     ]
    }
   ],
   "source": [
    "func extractMotionFeatures(_ batchTensor: Tensor<Float>, resnet: ResNet) -> Tensor<Float> {\n",
    "    // sliding resnet feature extractor\n",
    "    var t2: [Tensor<Float>] = []\n",
    "    let origBatchSize = batchTensor.shape[0]\n",
    "    let nElements = (tensorWidth/stride)-1\n",
    "    for i in 0..<nElements {\n",
    "        let start = i*stride\n",
    "        let end = i*stride+tWidth\n",
    "        // print(start, end)\n",
    "        let t1 = batchTensor[0..., start..<end]\n",
    "        // print(t1.shape)\n",
    "        t2.append(t1)\n",
    "    }\n",
    "    let t3 = Tensor(concatenating: t2)\n",
    "    // print(t3.shape)\n",
    "    let emb2 = resnet(t3)\n",
    "    let outShape: Array<Int> = [origBatchSize, nElements, resnet.classifier.weight.shape[1]]\n",
    "    // print(outShape)\n",
    "    let emb3 = emb2.reshaped(to: TensorShape(outShape))\n",
    "    return emb3\n",
    "}\n",
    "var emb3: Tensor<Float>? = nil\n",
    "time() {\n",
    "    print(batchTensor.shape)\n",
    "    emb3 = extractMotionFeatures(batchTensor, resnet: resnet)\n",
    "    print(emb3!.shape)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny BERT/Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "// let workspaceURL = URL(\n",
    "//     fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "//     relativeTo: URL(\n",
    "//         fileURLWithPath: NSTemporaryDirectory(),\n",
    "//         isDirectory: true))\n",
    "// let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "// var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var caseSensitive: Bool = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "var subDirectory: String = \"uncased_L-12_H-768_A-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "let directory = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "let vocabularyURL = directory\n",
    "    .appendingPathComponent(subDirectory)\n",
    "    .appendingPathComponent(\"vocab.txt\")\n",
    "\n",
    "let vocabulary: Vocabulary = try! Vocabulary(fromFile: vocabularyURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "let tokenizer: Tokenizer = try \n",
    "                     BERTTokenizer(\n",
    "                        vocabulary: vocabulary,\n",
    "                        caseSensitive: caseSensitive,\n",
    "                        unknownToken: \"[UNK]\",\n",
    "                        maxTokenLength: nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "var variant: BERT.Variant = .bert          \n",
    "var hiddenSize: Int = 768\n",
    "var hiddenLayerCount: Int = 12\n",
    "var attentionHeadCount: Int = 12\n",
    "var intermediateSize: Int = hiddenSize*4 // 3072/768=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func createAttentionMask2(input2: Tensor<Float>, mask: Tensor<Int32>) -> Tensor<Float> {\n",
    "//     let batchSize = text.tokenIds.shape[0]\n",
    "    let batchSize = input2.shape[0]\n",
    "//     let fromSequenceLength = text.tokenIds.shape[1]\n",
    "//     let toSequenceLength = text.mask.shape[1]\n",
    "    let fromSequenceLength = input2.shape[1]\n",
    "    let toSequenceLength = input2.shape[1]\n",
    "//     let reshapedMask = Tensor<Float>(text.mask.reshaped(to: [batchSize, 1, toSequenceLength]))\n",
    "    let reshapedMask = Tensor<Float>(mask.reshaped(to: [batchSize, 1, toSequenceLength]))\n",
    "\n",
    "    // We do not assume that `input.tokenIds` is a mask. We do not actually care if we attend\n",
    "    // *from* padding tokens (only *to* padding tokens) so we create a tensor of all ones.\n",
    "//     let broadcastOnes = Tensor<Float>(ones: [batchSize, fromSequenceLength, 1], on: text.mask.device)\n",
    "    let broadcastOnes = Tensor<Float>(ones: [batchSize, fromSequenceLength, 1], on: mask.device)\n",
    "\n",
    "    // We broadcast along two dimensions to create the mask.\n",
    "    return broadcastOnes * reshapedMask\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension BERT {\n",
    "        @differentiable(wrt: self)\n",
    "    public func callAsFunction(_ input2: Tensor<Float>) -> Tensor<Scalar> {\n",
    "        print(\"ala ma kota\")\n",
    "        print(\"input2 = \\(input2.shape)\")\n",
    "//         let tokenIds: Tensor<Int32> = Tensor<Int32>([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "//         let tokenTypeIds: Tensor<Int32> = Tensor<Int32>([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
    "        let mask: Tensor<Int32> = Tensor<Int32>([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]])\n",
    "//         let input: TextBatch = TextBatch(tokenIds: tokenIds, tokenTypeIds: tokenTypeIds, mask: mask)\n",
    "//         let sequenceLength = input.tokenIds.shape[1]\n",
    "        let sequenceLength = input2.shape[1]\n",
    "//         let variant = withoutDerivative(at: self.variant)\n",
    "        print(1)\n",
    "\n",
    "        // Compute the input embeddings and apply layer normalization and dropout on them.\n",
    "//         let tokenEmbeddings = tokenEmbedding(input.tokenIds)\n",
    "        let tokenEmbeddings = input2\n",
    "\n",
    "        print(\"tokenEmbeddings: \\(tokenEmbeddings.shape)\")\n",
    "        \n",
    "//         let tokenTypeEmbeddings = tokenTypeEmbedding(input.tokenTypeIds)\n",
    "        let positionPaddingIndex: Int\n",
    "        \n",
    "        positionPaddingIndex = 0\n",
    "        \n",
    "        let positionEmbeddings = positionEmbedding.embeddings.slice(\n",
    "            lowerBounds: [positionPaddingIndex, 0],\n",
    "            upperBounds: [positionPaddingIndex + sequenceLength, -1]\n",
    "        ).expandingShape(at: 0)\n",
    "        \n",
    "        \n",
    "        print(\"positionEmbeddings: \\(positionEmbeddings.shape)\")\n",
    "        \n",
    "        var embeddings = tokenEmbeddings + positionEmbeddings\n",
    "\n",
    "        // Add token type embeddings if needed, based on which BERT variant is being used.\n",
    "//         embeddings = embeddings + tokenTypeEmbeddings\n",
    "\n",
    "        embeddings = embeddingLayerNorm(embeddings)\n",
    "        embeddings = embeddingDropout(embeddings)\n",
    "\n",
    "        // TODO: do masking, but outside\n",
    "        // TODO: get mask from 45th row (motion flag)\n",
    "//         let mask = Tensor<Int32>([Int32](repeating: 1, count: sequenceLength))\n",
    "        // Create an attention mask for the inputs with shape\n",
    "        // `[batchSize, sequenceLength, sequenceLength]`.\n",
    "//         let attentionMask = createAttentionMask(forTextBatch: input)\n",
    "        let attentionMask = createAttentionMask2(input2: input2, mask: mask)\n",
    "\n",
    "        // We keep the representation as a 2-D tensor to avoid reshaping it back and forth from a\n",
    "        // 3-D tensor to a 2-D tensor. Reshapes are normally free on GPUs/CPUs but may not be free\n",
    "        // on TPUs, and so we want to minimize them to help the optimizer.\n",
    "        var transformerInput = embeddings.reshapedToMatrix()\n",
    "        let batchSize = embeddings.shape[0]\n",
    "\n",
    "        // Run the stacked transformer.\n",
    "        for layerIndex in 0..<(withoutDerivative(at: encoderLayers) { $0.count }) {\n",
    "            transformerInput = encoderLayers[layerIndex](TransformerInput(\n",
    "            sequence: transformerInput,\n",
    "            attentionMask: attentionMask,\n",
    "            batchSize: batchSize))\n",
    "        }\n",
    "\n",
    "        // Reshape back to the original tensor shape.\n",
    "        return transformerInput.reshapedFromMatrix(originalShape: embeddings.shape)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bert = BERT(\n",
    "    variant: variant,\n",
    "    vocabulary: vocabulary,\n",
    "    tokenizer: tokenizer,\n",
    "    caseSensitive: caseSensitive,\n",
    "    hiddenSize: hiddenSize,\n",
    "    hiddenLayerCount: hiddenLayerCount,\n",
    "    attentionHeadCount: attentionHeadCount,\n",
    "    intermediateSize: intermediateSize,\n",
    "    intermediateActivation: gelu,\n",
    "    hiddenDropoutProbability: 0.1,\n",
    "    attentionDropoutProbability: 0.1,\n",
    "    maxSequenceLength: 512,\n",
    "    typeVocabularySize: 2,\n",
    "    initializerStandardDeviation: 0.02,\n",
    "    useOneHotEmbeddings: false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ala ma kota\n",
      "input2 = [2, 5, 768]\n",
      "1\n",
      "tokenEmbeddings: [2, 5, 768]\n",
      "positionEmbeddings: [1, 5, 768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "▿ [2, 5, 768]\n",
       "  ▿ dimensions : 3 elements\n",
       "    - 0 : 2\n",
       "    - 1 : 5\n",
       "    - 2 : 768\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(emb3!).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain resnet and bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Starting motion2label training...\")\n",
    "\n",
    "for epoch in 1...10 {\n",
    "//     print(\"epoch \\(epoch)\")\n",
    "    Context.local.learningPhase = .training\n",
    "    dataset.newTrainCrops()\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    for batch in dataset.training.sequenced() {\n",
    "        print(\"progress \\(100.0*Float(trainingBatchCount)/Float(dataset.training.count))%\")\n",
    "        let (tensors, labels) = (batch.first, batch.second)\n",
    "        let (loss, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in\n",
    "            let logits = model(tensors)\n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&model, along: gradients)\n",
    "    }\n",
    "\n",
    "    Context.local.learningPhase = .inference\n",
    "    var testLossSum: Float = 0\n",
    "    var testBatchCount = 0\n",
    "    var correctGuessCount = 0\n",
    "    var totalGuessCount = 0\n",
    "    for batch in dataset.test.sequenced() {\n",
    "//         print(\"batch\")\n",
    "        let (tensors, labels) = (batch.first, batch.second)\n",
    "        let logits = model(tensors)\n",
    "        testLossSum += softmaxCrossEntropy(logits: logits, labels: labels).scalarized()\n",
    "        testBatchCount += 1\n",
    "\n",
    "        let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
    "        correctGuessCount = correctGuessCount\n",
    "            + Int(\n",
    "                Tensor<Int32>(correctPredictions).sum().scalarized())\n",
    "        totalGuessCount = totalGuessCount + batchSize\n",
    "    }\n",
    "\n",
    "    let accuracy = Float(correctGuessCount) / Float(totalGuessCount)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        [Epoch \\(epoch)] \\\n",
    "        Training loss: \\(trainingLossSum  / Float(trainingBatchCount)) \\\n",
    "        Accuracy: \\(correctGuessCount)/\\(totalGuessCount) (\\(accuracy*100)%) \\\n",
    "        Loss: \\(testLossSum / Float(testBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
