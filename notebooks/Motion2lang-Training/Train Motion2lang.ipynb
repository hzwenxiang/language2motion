{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Motion 2 language transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt\")' Datasets TranslationModels TextModels ModelSupport SummaryWriter MotionModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "import TextModels\n",
    "import TranslationModels\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import Datasets\n",
    "import SummaryWriter\n",
    "import MotionModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let runName = \"run_1\"\n",
    "// let batchSize = 6000\n",
    "// let batchSize = 3000\n",
    "let batchSize = 200\n",
    "let maxSequenceLength =  50\n",
    "let nEpochs = 40\n",
    "// let learningRate: Float = 2e-5\n",
    "let learningRate: Float = 5e-4\n",
    "\n",
    "print(\"runName: \\(runName)\")\n",
    "print(\"batchSize: \\(batchSize)\")\n",
    "print(\"maxSequenceLength: \\(maxSequenceLength)\")\n",
    "print(\"nEpochs: \\(nEpochs)\")\n",
    "print(\"learningRate: \\(learningRate)\")\n",
    "\n",
    "let dataURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/\")\n",
    "// let motionDatasetURL = dataURL.appendingPathComponent(\"motion_dataset_v3.norm.10Hz.plist\")\n",
    "let motionDatasetURL = dataURL.appendingPathComponent(\"motion_dataset.motion_flag.normalized.downsampled.sampled.490.plist\")\n",
    "let langDatasetURL = dataURL.appendingPathComponent(\"labels_ds_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select eager or X10 backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let device = Device.defaultXLA\n",
    "let device = Device.defaultTFEager\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X10 warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// X10 warmup\n",
    "let eagerTensor1 = Tensor([0.0, 1.0, 2.0])\n",
    "let eagerTensor2 = Tensor([1.5, 2.5, 3.5])\n",
    "let eagerTensorSum = eagerTensor1 + eagerTensor2\n",
    "print(eagerTensorSum)\n",
    "print(eagerTensor1.device)\n",
    "let x10Tensor2 = Tensor([1.5, 2.5, 3.5], on: Device.defaultXLA)\n",
    "print(x10Tensor2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// instantiate text processor\n",
    "let vocabularyURL = dataURL.appendingPathComponent(\"vocab.txt\")\n",
    "let vocabulary: Vocabulary = try! Vocabulary(fromFile: vocabularyURL)\n",
    "let tokenizer: Tokenizer = BERTTokenizer(vocabulary: vocabulary, caseSensitive: false, unknownToken: \"[UNK]\", maxTokenLength: nil)\n",
    "let textProcessor = TextProcessor(vocabulary: vocabulary, tokenizer: tokenizer, maxSequenceLength: maxSequenceLength)\n",
    "\n",
    "// instantiate model\n",
    "let sourceVocabSize = vocabulary.count\n",
    "let inputSize = 48 // TODO: get value from dataset\n",
    "let targetVocabSize = vocabulary.count\n",
    "let layerCount: Int = 6\n",
    "let modelSize: Int = 256\n",
    "let feedForwardSize: Int = 1024\n",
    "let headCount: Int = 8\n",
    "let dropoutProbability: Double = 0.1\n",
    "\n",
    "var model = MotionLangTransformer(\n",
    "    sourceVocabSize: sourceVocabSize, \n",
    "    inputSize: inputSize,\n",
    "    targetVocabSize: targetVocabSize,\n",
    "    layerCount: layerCount, \n",
    "    modelSize: modelSize, \n",
    "    feedForwardSize: feedForwardSize, \n",
    "    headCount: headCount, \n",
    "    dropoutProbability: dropoutProbability\n",
    ")\n",
    "\n",
    "model.move(to: device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "\n",
    "var dataset = try Motion2Lang(\n",
    "    motionDatasetURL: motionDatasetURL,\n",
    "    langDatasetURL: langDatasetURL,\n",
    "    maxSequenceLength: maxSequenceLength,\n",
    "    batchSize: batchSize\n",
    ") { (example: Motion2Lang.Example) -> MotionLangBatch in    \n",
    "    let singleBatch = textProcessor.preprocess(example: example)\n",
    "    return singleBatch\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")\n",
    "\n",
    "// get example\n",
    "let example = dataset.trainExamples[0]\n",
    "print(\"example.id: \\(example.id)\")\n",
    "print(\"example.motionSample.timestepsArray.last: \\(example.motionSample.timestepsArray.last!)\")\n",
    "print(\"example.motionSample.motionFramesArray.shape: \\(example.motionSample.motionFramesArray.shape)\")\n",
    "print(\"example.targetSentence: \\(example.targetSentence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// get a batch\n",
    "// print(\"\\nOne batch (MotionLangBatch):\")\n",
    "// var epochIterator = dataset.trainingEpochs.enumerated().makeIterator()\n",
    "// let epoch = epochIterator.next()\n",
    "// let batches = Array(epoch!.1)\n",
    "// let batch: MotionLangBatch = batches[0]\n",
    "// print(\"type: \\(type(of:batch))\")\n",
    "// print(\"motionFrames.shape: \\(batch.motionFrames.shape)\")\n",
    "// // print(\"motionFlag.shape: \\(batch.motionFlag.shape)\")\n",
    "// print(\"mask.shape: \\(batch.mask.shape)\")\n",
    "// print(\"origMotionFramesCount.shape: \\(batch.origMotionFramesCount.shape)\")\n",
    "// print(\"origMotionFramesCount: \\(batch.origMotionFramesCount)\")\n",
    "// print(\"targetTokenIds.shape: \\(batch.targetTokenIds.shape)\")\n",
    "// print(\"targetMask.shape: \\(batch.targetMask.shape)\")\n",
    "// print(\"targetTruth.shape: \\(batch.targetTruth.shape)\")\n",
    "\n",
    "// run one batch\n",
    "// print(\"\\nRun one batch:\")\n",
    "// print(\"==============\")\n",
    "// let deviceBatch = MotionLangBatch(copying: batch, to: device)\n",
    "// let output = model(deviceBatch)\n",
    "// print(\"output.shape: \\(output.shape)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = Adam(for: model, learningRate: learningRate)\n",
    "optimizer = Adam(copying: optimizer, to: device)\n",
    "\n",
    "let logdirURL = dataURL.appendingPathComponent(\"tboard/Motion2lang/\\(runName)\", isDirectory: true)\n",
    "let summaryWriter = SummaryWriter(logdir: logdirURL, flushMillis: 30*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable(wrt: logits)\n",
    "public func softmaxCrossEntropy2(logits: Tensor<Float>, labels: Tensor<Int32>, ignoreIndex: Int32) -> Tensor<Float> {\n",
    "    // print(\"softmaxCrossEntropy2() - start\")\n",
    "    // FIXME: use logits.device, move code back to Utilities.swift\n",
    "    // print(\"  LazyTensorBarrier()\")\n",
    "    // time {\n",
    "    //     LazyTensorBarrier()\n",
    "    // }\n",
    "    // let ids = Tensor<Int32>(rangeFrom: 0, to: Int32(labels.shape.first!), stride: 1, on: device)    \n",
    "    // let indices = ids.gathering(where: labels .!= Tensor(ignoreIndex, on: device))\n",
    "    // let maskedLogits = logits.gathering(atIndices: indices, alongAxis: 0)\n",
    "    // let maskedTargets = labels.gathering(atIndices: indices, alongAxis: 0)\n",
    "    // print(\"  maskedLogits.shape: \\(maskedLogits.shape)\")\n",
    "    // print(\"  maskedTargets.shape: \\(maskedTargets.shape)\")\n",
    "    let sce = softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "    // print(\"sce: \\(sce)\")\n",
    "    // let maskedSCE = softmaxCrossEntropy(logits: maskedLogits, labels: maskedTargets)\n",
    "    // print(\"maskedSCE: \\(maskedSCE)\")\n",
    "    // print(\"softmaxCrossEntropy2() - stop\")\n",
    "    return sce\n",
    "}\n",
    "\n",
    "func update(model: inout MotionLangTransformer, using optimizer: inout Adam<MotionLangTransformer>, for batch: MotionLangBatch) -> Float {\n",
    "    // print(\"update() - start\")\n",
    "    let labels = batch.targetTruth.reshaped(to: [-1])\n",
    "    let resultSize = batch.targetTruth.shape.last! * batch.targetTruth.shape.first!\n",
    "    // print(\"  resultSize: \\(resultSize)\")\n",
    "    let padIndex = textProcessor.padId\n",
    "    let result = withLearningPhase(.training) { () -> Float in\n",
    "        let (loss, grad) = valueWithGradient(at: model) {\n",
    "            (model) -> Tensor<Float> in\n",
    "            let logits = model.generate(input: batch).reshaped(to: [resultSize, -1])\n",
    "            // print(\"  logits.shape: \\(logits.shape)\")\n",
    "            // print(\"  labels.shape: \\(labels.shape)\")\n",
    "            let sce = softmaxCrossEntropy2(logits: logits, labels: labels,ignoreIndex: padIndex)\n",
    "            return sce\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)\n",
    "        LazyTensorBarrier()\n",
    "        return loss.scalarized()\n",
    "    }\n",
    "    // print(\"update() - stop\")\n",
    "    return result\n",
    "}\n",
    "\n",
    "/// returns validation loss\n",
    "func validate(model: inout MotionLangTransformer, for batch: MotionLangBatch) -> Float {\n",
    "    let labels = batch.targetTruth.reshaped(to: [-1])\n",
    "    let resultSize = batch.targetTruth.shape.last! * batch.targetTruth.shape.first!\n",
    "    let padIndex = textProcessor.padId\n",
    "    let result = withLearningPhase(.inference) { () -> Float in\n",
    "        softmaxCrossEntropy2(logits: model.generate(input: batch).reshaped(to: [resultSize, -1]), labels: labels,ignoreIndex: padIndex).scalarized()\n",
    "    }\n",
    "    LazyTensorBarrier()\n",
    "    return result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func greedyDecode(model: MotionLangTransformer, input: MotionLangBatch, maxLength: Int, startSymbol: Int32) -> Tensor<Int32> {\n",
    "    let memory = model.encode(input: input)\n",
    "    var ys = Tensor(repeating: startSymbol, shape: [1,1])\n",
    "    // ys = Tensor(copying: ys, to: device)\n",
    "    for _ in 0..<maxLength {\n",
    "        let decoderInput = MotionLangBatch(motionFrames: input.motionFrames,\n",
    "                                     mask: input.mask,\n",
    "                                     origMotionFramesCount: input.origMotionFramesCount,\n",
    "                                     targetTokenIds: ys,\n",
    "                                     targetMask: Tensor<Float>(subsequentMask(size: ys.shape[1])),\n",
    "                                     targetTruth: input.targetTruth)\n",
    "        // decoderInput = MotionLangBatch(copying: decoderInput, to: device)\n",
    "        let out = model.decode(input: decoderInput, memory: memory)\n",
    "        let prob = model.generate(input: out[0...,-1])\n",
    "        let nextWord = Int32(prob.argmax().scalarized())\n",
    "        ys = Tensor(concatenating: [ys, Tensor(repeating: nextWord, shape: [1,1])], alongAxis: 1) // , on: device\n",
    "        // ys = Tensor(copying: ys, to: device)\n",
    "    }\n",
    "    return ys\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// setup decoding\n",
    "var epochIterator2 = dataset.trainingEpochs.enumerated().makeIterator()\n",
    "let epoch2 = epochIterator2.next()\n",
    "let batches2 = Array(epoch2!.1)\n",
    "let batch2 = batches2[0]\n",
    "\n",
    "let exampleIndex = 1 // FIXME: utilize exampleIndex\n",
    "// var source = batch2 //Motion2Lang.reduceDataBatches(batches2)\n",
    "\n",
    "let oneExample = dataset.trainExamples[0]\n",
    "let singleExampleBatch = textProcessor.preprocess(example: oneExample)\n",
    "var source = Motion2Lang.reduceDataBatches([singleExampleBatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var outputStr = textProcessor.decode(tensor: source.targetTokenIds)\n",
    "print(\"decode(source.targetTokenIds): \\(outputStr)\")\n",
    "\n",
    "Context.local.learningPhase = .inference\n",
    "source = MotionLangBatch(copying: source, to: Device.defaultTFEager)\n",
    "model.move(to: Device.defaultTFEager)\n",
    "let out = greedyDecode(model: model, input: source, maxLength: 50, startSymbol: textProcessor.bosId)\n",
    "outputStr = textProcessor.decode(tensor: out)\n",
    "print(\"greedyDecode(): \\\"\\(outputStr)\\\"\")\n",
    "model.move(to: device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Transformer for the Motion2lang task!\")\n",
    "var trainingStepCount = 0\n",
    "time() {\n",
    "    LazyTensorBarrier()\n",
    "    for (epoch, epochBatches) in dataset.trainingEpochs.prefix(nEpochs).enumerated() {\n",
    "        print(\"[Epoch \\(epoch + 1)]\")\n",
    "        Context.local.learningPhase = .training\n",
    "        var trainingLossSum: Float = 0\n",
    "        var trainingBatchCount = 0\n",
    "        if epoch == 0 {\n",
    "            print(\"epochBatches.count: \\(epochBatches.count)\")\n",
    "        }\n",
    "\n",
    "        for eagerBatch in epochBatches {\n",
    "            // print(\"==> step \\(trainingStepCount)\")\n",
    "            // print(\"eagerBatch.tokenIds.shape: \\(eagerBatch.tokenIds.shape)\")\n",
    "            // print(\"eagerBatch.targetTokenIds.shape: \\(eagerBatch.targetTokenIds.shape)\")\n",
    "            // print(\"eagerBatch.mask.shape: \\(eagerBatch.mask.shape)\")\n",
    "            // print(\"eagerBatch.targetTruth.shape: \\(eagerBatch.targetTruth.shape)\")\n",
    "            // print(\"eagerBatch.tokenCount: \\(eagerBatch.tokenCount)\")\n",
    "            let batch = MotionLangBatch(copying: eagerBatch, to: device)\n",
    "            let loss: Float = update(model: &model, using: &optimizer, for: batch)\n",
    "            // print(\"current loss at step \\(trainingStepCount): \\(loss)\")\n",
    "            trainingLossSum += loss\n",
    "            trainingBatchCount += 1\n",
    "            summaryWriter.writeScalarSummary(tag: \"TrainingLoss\", step: trainingStepCount, value: trainingLossSum / Float(trainingBatchCount))\n",
    "            trainingStepCount += 1\n",
    "        }\n",
    "        print(\n",
    "            \"\"\"\n",
    "            Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "        summaryWriter.writeScalarSummary(tag: \"EpochTrainingLoss\", step: epoch+1, value: trainingLossSum / Float(trainingBatchCount))\n",
    "\n",
    "        if epoch == 0 {\n",
    "            print(\"dataset.validationBatches.count: \\(dataset.validationBatches.count)\")\n",
    "        }\n",
    "        Context.local.learningPhase = .inference\n",
    "        var devLossSum: Float = 0\n",
    "        var devBatchCount = 0\n",
    "        var totalGuessCount = 0\n",
    "\n",
    "        for eagerBatch in dataset.validationBatches {\n",
    "            let batch = MotionLangBatch(copying: eagerBatch, to: device)\n",
    "            let loss: Float = validate(model: &model, for: batch)\n",
    "            let valBatchSize = batch.motionFrames.shape[0]\n",
    "\n",
    "            devLossSum += loss\n",
    "            devBatchCount += 1\n",
    "            totalGuessCount += valBatchSize\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "            totalGuessCount: \\(totalGuessCount) \\\n",
    "            Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "        summaryWriter.writeScalarSummary(tag: \"EpochTestLoss\", step: epoch+1, value: devLossSum / Float(devBatchCount))\n",
    "\n",
    "        print(\"\\nEncoding/decoding one example\") // on eager device\n",
    "        Context.local.learningPhase = .inference\n",
    "        source = MotionLangBatch(copying: source, to: Device.defaultTFEager)\n",
    "        model.move(to: Device.defaultTFEager)\n",
    "        let out = greedyDecode(model: model, input: source, maxLength: 50, startSymbol: textProcessor.bosId)\n",
    "        outputStr = textProcessor.decode(tensor: out)\n",
    "        print(\"greedyDecode(): \\\"\\(outputStr)\\\"\")\n",
    "        model.move(to: device)\n",
    "    }\n",
    "    summaryWriter.flush()\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// encode/decode one example\n",
    "// print(\"\\nEncoding/decoding one example\")\n",
    "// Context.local.learningPhase = .inference\n",
    "// source = MotionLangBatch(copying: source, to: device)\n",
    "// model.move(to: Device.defaultTFEager)\n",
    "// let out = greedyDecode(model: model, input: source, maxLength: 50, startSymbol: textProcessor.bosId)\n",
    "// outputStr = textProcessor.decode(tensor: out)\n",
    "// print(\"greedyDecode(), outputStr: \\(outputStr)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
